{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--gamma G] [--seed N] [--render]\n",
      "                             [--log-interval N]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /run/user/1001/jupyter/kernel-a04119f9-98d0-4b86-9b59-15c09358b989.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jul/miniconda3/envs/dl/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                    help='random seed (default: 543)')\n",
    "parser.add_argument('--render', action='store_true',\n",
    "                    help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='interval between training status logs (default: 10)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = self.action_head(x)\n",
    "        state_values = self.value_head(x)\n",
    "        return F.softmax(action_scores, dim=-1), state_values\n",
    "\n",
    "\n",
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    returns = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + args.gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "    optimizer.zero_grad()\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % args.log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
